{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\mychat\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\mychat\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\mychat\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.7.24-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in .\\mychat\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.4-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\mychat\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\mychat\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in .\\mychat\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\mychat\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\mychat\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\mychat\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\mychat\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/9.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.6/9.5 MB 3.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.4/9.5 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.1/9.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.2/9.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.5/9.5 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.3/9.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.8/9.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.5 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.5 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.5 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 2.9 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Using cached regex-2024.7.24-cp310-cp310-win_amd64.whl (269 kB)\n",
      "Downloading safetensors-0.4.4-cp310-none-win_amd64.whl (285 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.24.5 regex-2024.7.24 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-docx) (5.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install python-docx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.24.9)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from PyMuPDF) (1.24.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install  PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! Ask me anything about the document.\n",
      "Chatbot: working on desktops user-friendly interface designed to enhance usability and streamline classroom management People: The people section The navigation bar Teams, Zoom or, Google Meet.\n",
      "Click Save Managing Class courses\n",
      "Managing a course in a class online lecture online exam First click on Edit Click Save New chat room grade level limiting access to only the students enrolled in that class Whether you are creating a new program or reorganizing an existing one Studies\" section in the navigation bar proctoring General tab 6-Save: Click \"Save\" Invoice Number: Unique identifier school fees\n",
      "Chatbot: working on desktops a summary of the total number of classes and students in your account you can manage classes, courses, and admissions Clicking on a class card Click on the three vertical dots Single Select option face to face class online and offline exams import the grades in the gradebook Click Save New chat room click on the \"New Course\" button public class click on the \"New Program\" button click on the \"New Campaign\" button by clicking on the \"New Exam\" button the Accounts page Click on View profile automatically generated school fees\n",
      "Chatbot: working on desktops an admin account you can manage classes, courses, and admissions Clicking on a class card Click on the three vertical dots Single Select option above and unchecking the student from the list manually online and offline exams import the grades in the gradebook Click Save New chat room click on the \"New Course\" button define skills to gain click on the \"New Program\" button click on the \"New Campaign\" button Create new admission exams the Accounts page Click on \"Preview\" Click on the \"cash\" icon school fees\n",
      "Chatbot: The question cannot be empty.\n",
      "Chatbot: The question cannot be empty.\n",
      "Chatbot: The question cannot be empty.\n",
      "Chatbot: Goodbye!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from docx import Document\n",
    "import csv\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Function to read the document and extract text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a .docx file and return it as a single string.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    text = [para.text for para in doc.paragraphs]\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read the document and extract text from a .csv file\n",
    "def read_csv(file_path):\n",
    "    \"\"\"Read text from a .csv file and return it as a single string.\"\"\"\n",
    "    text = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            text.append(','.join(row))\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read the document and extract text from a .pdf file\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Read text from a .pdf file and return it as a single string.\"\"\"\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Function to split the text into manageable chunks\n",
    "def chunk_text(text, chunk_size=2000):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to answer questions using the QA pipeline\n",
    "def answer_question(question, context_chunks):\n",
    "    if not question.strip():\n",
    "        return \"The question cannot be empty.\"\n",
    "\n",
    "    answers = []\n",
    "    for chunk in context_chunks:\n",
    "        result = qa_pipeline(question=question, context=chunk)\n",
    "        answers.append(result.get('answer', 'No answer found.'))\n",
    "\n",
    "    combined_answer = \" \".join(answers)\n",
    "    return combined_answer if combined_answer else \"No relevant information found.\"\n",
    "\n",
    "# Initialize the question-answering pipeline with a pre-trained model\n",
    "qa_pipeline = pipeline(\"question-answering\") \n",
    "\n",
    "# Read all .docx, .csv, and .pdf files in the specified folder and combine their content\n",
    "def read_folder(folder_path):\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.docx'):\n",
    "            combined_text += read_docx(file_path) + \"\\n\"\n",
    "        elif filename.endswith('.csv'):\n",
    "            combined_text += read_csv(file_path) + \"\\n\"\n",
    "        elif filename.endswith('.pdf'):\n",
    "            combined_text += read_pdf(file_path) + \"\\n\"\n",
    "    return combined_text\n",
    "\n",
    "# Determine the correct folder path and read its content\n",
    "folder_path = 'C:\\\\chatbot\\\\documents'  # Adjust this path as needed\n",
    "document_text = read_folder(folder_path)\n",
    "\n",
    "# Split the document text into chunks\n",
    "chunks = chunk_text(document_text)\n",
    "\n",
    "def chatbot_interaction():\n",
    "    \"\"\"Start a conversation loop for the chatbot.\"\"\"\n",
    "    print(\"Chatbot: Hello! Ask me anything about the document.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        answer = answer_question(user_input, chunks)\n",
    "        print(\"Chatbot:\", answer)\n",
    "\n",
    "# Run the chatbot interaction\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_interaction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF for handling PDFs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import fitz  # PyMuPDF for handling PDFs\n",
    "from docx import Document\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Function to read the document and extract text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a .docx file and return it as a single string.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    text = [para.text for para in doc.paragraphs]\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read the document and extract text from a .csv file\n",
    "def read_csv(file_path):\n",
    "    \"\"\"Read text from a .csv file and return it as a single string.\"\"\"\n",
    "    text = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            text.append(','.join(row))\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read the document and extract text from a .pdf file\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Read text from a .pdf file and return it as a single string.\"\"\"\n",
    "    text = []\n",
    "    with fitz.open(file_path) as pdf:\n",
    "        for page_num in range(len(pdf)):\n",
    "            page = pdf.load_page(page_num)\n",
    "            text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read all documents from a folder and combine their content\n",
    "def read_folder(folder_path):\n",
    "    \"\"\"Read all supported documents in the specified folder and return combined text.\"\"\"\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.docx'):\n",
    "            combined_text += read_docx(file_path) + \"\\n\"\n",
    "        elif filename.endswith('.csv'):\n",
    "            combined_text += read_csv(file_path) + \"\\n\"\n",
    "        elif filename.endswith('.pdf'):\n",
    "            combined_text += read_pdf(file_path) + \"\\n\"\n",
    "    return combined_text\n",
    "\n",
    "# Function to split the text into manageable chunks\n",
    "def chunk_text(text, chunk_size=2000):\n",
    "    \"\"\"Split the text into chunks of a specified size.\"\"\"\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to answer questions using the QA pipeline\n",
    "def answer_question(question, context_chunks):\n",
    "    \"\"\"Answer a question based on the context chunks.\"\"\"\n",
    "    if not question.strip():\n",
    "        return \"The question cannot be empty.\"\n",
    "\n",
    "    answers = []\n",
    "    for chunk in context_chunks:\n",
    "        result = qa_pipeline(question=question, context=chunk)\n",
    "        answers.append(result.get('answer', 'No answer found.'))\n",
    "\n",
    "    combined_answer = \" \".join(answers)\n",
    "    return combined_answer if combined_answer else \"No relevant information found.\"\n",
    "\n",
    "# Initialize the question-answering pipeline with a pre-trained model\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Path to the folder containing your documents\n",
    "folder_path = 'C:\\\\chatbot\\\\documents'  # Adjust this path as needed\n",
    "document_text = read_folder(folder_path)\n",
    "\n",
    "# Split the document text into chunks\n",
    "chunks = chunk_text(document_text)\n",
    "\n",
    "# Function to handle chatbot interaction\n",
    "def chatbot_interaction():\n",
    "    \"\"\"Start a conversation loop for the chatbot.\"\"\"\n",
    "    print(\"Chatbot: Hello! Ask me anything about the documents.\")\n",
    "    \n",
    "    # Start a conversation loop\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        print(f\"User Input: '{user_input}'\")  # Debugging statement\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Check for empty input\n",
    "        if not user_input.strip():\n",
    "            print(\"Chatbot: Please ask a question.\")\n",
    "            continue\n",
    "\n",
    "        answer = answer_question(user_input, chunks)\n",
    "        print(\"Chatbot:\", answer)\n",
    "\n",
    "# Run the chatbot interaction\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_interaction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.generativeai\n",
      "  Downloading google_generativeai-0.7.2-py3-none-any.whl (164 kB)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Collecting google-api-core\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.139.0-py2.py3-none-any.whl (12.1 MB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-5.27.3-cp310-abi3-win_amd64.whl (426 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.6\n",
      "  Downloading google_ai_generativelanguage-0.6.6-py3-none-any.whl (718 kB)\n",
      "Requirement already satisfied: typing-extensions in .\\lib\\site-packages (from google.generativeai) (4.12.2)\n",
      "Collecting google-auth>=2.15.0\n",
      "  Downloading google_auth-2.32.0-py2.py3-none-any.whl (195 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.4-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Collecting requests<3.0.0.dev0,>=2.18.0\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting pydantic-core==2.20.1\n",
      "  Using cached pydantic_core-2.20.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: colorama in .\\lib\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl (14 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Downloading grpcio-1.65.4-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pyasn1, urllib3, rsa, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, requests, pyparsing, proto-plus, grpcio, googleapis-common-protos, google-auth, httplib2, grpcio-status, google-api-core, uritemplate, pydantic-core, google-auth-httplib2, annotated-types, tqdm, pydantic, google-api-python-client, google-ai-generativelanguage, google.generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.4.0 certifi-2024.7.4 charset-normalizer-3.3.2 google-ai-generativelanguage-0.6.6 google-api-core-2.19.1 google-api-python-client-2.139.0 google-auth-2.32.0 google-auth-httplib2-0.2.0 google.generativeai-0.7.2 googleapis-common-protos-1.63.2 grpcio-1.65.4 grpcio-status-1.62.3 httplib2-0.22.0 idna-3.7 proto-plus-1.24.0 protobuf-4.25.4 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.8.2 pydantic-core-2.20.1 pyparsing-3.1.2 requests-2.32.3 rsa-4.9 tqdm-4.66.5 uritemplate-4.1.1 urllib3-2.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'c:\\chatbot\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docx'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "import csv\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure Google Generative AI client\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)\n",
    "generation_config = {\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_p\": 1,\n",
    "    \"top_k\": 1,\n",
    "    \"max_output_token\": 2048\n",
    "}\n",
    "model = genai.GenerativeModel(\"gemini-pro\", generation_config=generation_config)\n",
    "\n",
    "# Function to read the document and extract text from a .docx file\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a .docx file and return it as a single string.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    text = [para.text for para in doc.paragraphs]\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read the document and extract text from a .csv file\n",
    "def read_csv(file_path):\n",
    "    \"\"\"Read text from a .csv file and return it as a single string.\"\"\"\n",
    "    text = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            text.append(','.join(row))\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read the document and extract text from a .pdf file\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Read text from a .pdf file and return it as a single string.\"\"\"\n",
    "    import fitz  # PyMuPDF for handling PDFs\n",
    "    text = []\n",
    "    with fitz.open(file_path) as pdf:\n",
    "        for page_num in range(len(pdf)):\n",
    "            page = pdf.load_page(page_num)\n",
    "            text.append(page.get_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Function to read all documents from a folder and combine their content\n",
    "def read_folder(folder_path):\n",
    "    \"\"\"Read all supported documents in the specified folder and return combined text.\"\"\"\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.docx'):\n",
    "            combined_text += read_docx(file_path) + \"\\n\"\n",
    "        elif filename.endswith('.csv'):\n",
    "            combined_text += read_csv(file_path) + \"\\n\"\n",
    "        elif filename.endswith('.pdf'):\n",
    "            combined_text += read_pdf(file_path) + \"\\n\"\n",
    "    return combined_text\n",
    "\n",
    "# Function to split the text into manageable chunks\n",
    "def chunk_text(text, chunk_size=2000):\n",
    "    \"\"\"Split the text into chunks of a specified size.\"\"\"\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to answer questions using the Google Generative AI model\n",
    "def answer_question(question, context_chunks):\n",
    "    \"\"\"Answer a question based on the context chunks using Generative AI.\"\"\"\n",
    "    if not question.strip():\n",
    "        return \"The question cannot be empty.\"\n",
    "\n",
    "    answers = []\n",
    "    for chunk in context_chunks:\n",
    "        response = model.generate(question + \" \" + chunk)\n",
    "        answers.append(response.text)\n",
    "\n",
    "    combined_answer = \" \".join(answers)\n",
    "    return combined_answer if combined_answer else \"No relevant information found.\"\n",
    "\n",
    "# Path to the folder containing your documents\n",
    "folder_path = 'C:\\\\chatbot\\\\documents'  # Adjust this path as needed\n",
    "document_text = read_folder(folder_path)\n",
    "\n",
    "# Split the document text into chunks\n",
    "chunks = chunk_text(document_text)\n",
    "\n",
    "# Function to handle chatbot interaction\n",
    "def chatbot_interaction():\n",
    "    \"\"\"Start a conversation loop for the chatbot.\"\"\"\n",
    "    print(\"Chatbot: Hello! Ask me anything about the documents.\")\n",
    "    \n",
    "    # Start a conversation loop\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        print(f\"User Input: '{user_input}'\")  # Debugging statement\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Check for empty input\n",
    "        if not user_input.strip():\n",
    "            print(\"Chatbot: Please ask a question.\")\n",
    "            continue\n",
    "\n",
    "        answer = answer_question(user_input, chunks)\n",
    "        print(\"Chatbot:\", answer)\n",
    "\n",
    "# Run the chatbot interaction\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_interaction()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
